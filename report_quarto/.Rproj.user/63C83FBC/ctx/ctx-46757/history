source("~/.active-rstudio-document", echo=TRUE)
#| label: Загрузка пакетов
#| warning: false
library(tidyverse)
library(readxl)
library(VIM) #для визуализации пропусков
library(rvest) #для копирования таблицы из веба
library(pracma) #для подсчета скользящей средней
#| label: reading_XXI
#| warning: false
reading_XXI <- function(name_of_df){
result <- read_excel(name_of_df) |>
select(1,2,28) |>
rename(Local_time = 1) |>
mutate(Local_time = as_datetime(Local_time, format="%d.%m.%Y %R")) |>
mutate(Year = year(Local_time),
Month = month(Local_time),
Day = mday(Local_time)) |>
rename(Sn = "E'",
Tavg = "T") |>
group_by(Year, Month, Day) |>
summarise(Sn = as.factor(str_flatten(Sn, ", ", na.rm = T)),
Tavg = mean(Tavg, na.rm=T)) |>
arrange(Year, Month, Day) |>
mutate(Station = str_match(name_of_df, "station/(.*?).xls")[2])
return(result)
}
#| label: Оставшиеся функции
#| warning: false
reading_XX <- function(x){read_table2(x, col_names = F)}
parsing_html_table <- function(http){
df <- read_html(http)
year <- df |>
html_element(".chronicle-table-left-column") |>
html_table() |>
slice(-1)
temper <- df |>
html_element(".chronicle-table") |>
html_table() |>
slice(-1)
result <- cbind(year, temper)
colnames(result) <- c("Year", c(1:12), "annualy")
result <- result |>
select(-annualy) |>
pivot_longer(-Year, values_to = "Tavg", names_to = "Month") |>
filter(Year %in% c(1961:2004) | (Year == 2005 & Month==1)) |>
mutate_at(c(1:3), .funs=as.numeric)
return(result)
}
std <- function(x){ sd(x)/sqrt(length(x)) } # Стандартная ошибка - стандартное отклонение/квадратный корень из числа наблюдений
# exponential_smooth <- function(x) { es(x, model="AAdN",h=8,holdout=FALSE,cfType="MSE")$fitted}
means_smooth <- function(x, n) {
pracma::movavg(x, n=10, type=c("s"))
}
#| label: Импорт
#| warning: false
# Данные 2005-2023 (6 станций(без бахты)) суточное разрешение ----------------------------------------------
paths <- list.files("initial_data/climate/2005_2023_seven_station", pattern = "[.]xls$", full.names = TRUE)
six_station_2005_2023 <- paths |>
map_dfr(reading_XXI)
rm(reading_XXI, paths)
aggr(six_station_2005_2023, prop = F, numbers = T) #отлично, пропусков нет
six_station_2005_2023_monthly <- six_station_2005_2023 |>
group_by(Station, Year, Month) |>
summarise(Tavg = mean(Tavg))
View(six_station_2005_2023_monthly)
levels(factor(six_station_2005_2023_monthly$Station))
paths <- list.files("initial_data/climate/monthly_all_station/", pattern = "[.]txt$", full.names = TRUE) # Просканировали все файлы в директории
four_station_obninsk <- paths |>
map(reading_XX) |>
list_rbind() |>
rename(Station = X1) |>
mutate(Station = case_when(
Station == "23274" ~"igarka",
Station == "23678" ~ "verkhneimbatsk",
Station == "23472" ~ "turukhansk",
.default = "bor"))
colnames(four_station_obninsk) <- c("Station", "Year", "1", "2", "3", "4", "5", "6", "7","8", "9", "10", "11", "12")
rm(paths)
four_station_obninsk <- four_station_obninsk |>
pivot_longer(-c(Station, Year), names_to = "Month", values_to = "Tavg") |>
mutate(Month = as.numeric(Month)) |>
filter(Year %in% c(1961:2004) | (Year == 2005 & Month==1))
View(four_station_obninsk)
#| label: температура Обнинск (месячное разрешение) 4 станции
#| warning: false
paths <- list.files("initial_data/climate/monthly_all_station/", pattern = "[.]txt$", full.names = TRUE) # Просканировали все файлы в директории
four_station_obninsk <- paths |>
map(reading_XX) |>
list_rbind() |>
rename(Station = X1) |>
mutate(Station = case_when(
Station == "23274" ~"igarka",
Station == "23678" ~ "verkhneimbatsk",
Station == "23472" ~ "turukhansk",
.default = "bor"))
colnames(four_station_obninsk) <- c("Station", "Year", "1", "2", "3", "4", "5", "6", "7","8", "9", "10", "11", "12")
rm(paths)
four_station_obninsk <- four_station_obninsk |>
pivot_longer(-c(Station, Year), names_to = "Month", values_to = "Tavg") |>
mutate(Month = as.numeric(Month)) |>
filter(Year %in% c(1961:2004) | (Year == 2005 & Month==1))
#| label: Ворогово и ярцево (из интернета)
#| warning: false
#  ----------------------------------------
vorogovo <- parsing_html_table("http://www.pogodaiklimat.ru/history/23973.htm") |>
mutate(Station = "vorogovo")
yartsevo <- parsing_html_table("http://www.pogodaiklimat.ru/history/23987.htm") |>
mutate(Station = "yartsevo")
#| label: Объединение данных
#| warning: false
# 6 станций (кроме Бахты). Данных об осадках и глубине снега нет.
six_station <- rbind(six_station_2005_2023_monthly, four_station_obninsk, vorogovo, yartsevo) |>
arrange(Station, Year, Month) |>
filter(Year %in% c(1961:2022) | (Year %in% c(1961:2023) & Month <7)) |>
mutate(Pr = NA, Sn = NA) |>
select(Station, Year, Month, Tavg, Pr, Sn)
# предобработанные ранее данные из Бахты
bakhta_monthly <- read_csv2("initial_data/climate/cleaned/Bakhta_monthly_1966-1976_replaced_by_average_Bor_Verkhn.csv") |>
mutate(Station = "bakhta") |>
select(Station, Year, Month, Tavg, Pr, Sn)
# данные по всем метеостанциям
all_station <- rbind(six_station, bakhta_monthly) |>
arrange(Station, Year, Month)
average_temp_by_7_station <- all_station |>
group_by(Year, Month) |>
summarise(Tavg = mean(Tavg)) |>
mutate(Station = "average_by_all_station")
# данные по всем метеостанциям + осредненные
seven_station <- rbind(all_station, average_temp_by_7_station)
# данные о характеристиках снега по 6 станциям (кроме Бахты)
snow_quality_six_station <- six_station_2005_2023 |>
select(-Tavg) |>
rename(Sn_description = Sn)
write_csv2(snow_quality_six_station, "initial_data/climate/cleaned/snow_quality_six_station.csv")
rm(all_station, average_temp_by_7_station, bakhta_monthly, four_station_obninsk, six_station,
six_station_2005_2023,    six_station_2005_2023_monthly, vorogovo, yartsevo)
# Расчет аномалий, среднего базового и тд ---------------------------------
# Осреднение --------------------------------------------------------------
annualy_T_avg_Pr <- seven_station |>
rename(T_=Tavg) |> # не менять, иначе не будут считаться SE
group_by(Station, Year) |>
summarise(Tavg = mean(T_),
Tavg_SE = std(T_),
Pr = sum(Pr)) # SE для Pr не считаем, так как там не среднее, а сумма
#Для снежного покрова сделаем отдельный расчет среднегодового, не учитывая летние месяцы
annualy_Sn <- seven_station |>
rename(Sn_=Sn) |> # не менять, иначе не будут считаться SE
filter(Month %in% c(1:5,9:12)) |>
group_by(Station, Year) |>
summarise(Sn = mean(Sn_),
Sn_SE = std(Sn_))
annualy <- annualy_T_avg_Pr |>
left_join(annualy_Sn, by = c("Station", "Year"))
rm(annualy_Sn, annualy_T_avg_Pr, six_station_monthly, Bakhta_monthly, tavg_by_seven_station)
# Расчет базовых (опорных) значений ---------------------------------------
base_annualy <- annualy |>
filter(Year %in% c(1961:1990)) |>
group_by(Station) |>
summarise(across(c(Tavg, Sn, Pr), list(
base = mean,
base_SE = std
)))
base_monthly <- seven_station |>
filter(Year %in% c(1961:1990)) |>
group_by(Station, Month) |>
summarise(across(c(Tavg, Sn, Pr), list(
base = mean,
base_SE = std
)))
# Средние значения за последнее десятилетие -------------------------------
last_decade_annualy <- annualy |>
filter(Year %in% c(2013:2022)) |>  #2023 год не берем, так как нет полных данных
group_by(Station) |>
summarise(across(c(Tavg, Sn, Pr), list(
last_decade = mean,
last_decade_SE = std
)))
last_decade_monthly <- seven_station |>
filter(Year %in% c(2013:2022)) |>
group_by(Station, Month) |>
summarise(across(c(Tavg, Sn, Pr), list(
last_decade = mean,
last_decade_SE = std
)))
# Расчет аномалий, скользящего среднего, линейного тренда с 1976 года и разницы между базовой и последним десятилетием --------
annualy_anomaly <- annualy |>
left_join(base_annualy, by = "Station") |>
group_by(Station) |>
mutate(T_anomaly = Tavg-Tavg_base,
Sn_anomaly = Sn-Sn_base,
Pr_anomaly = Pr-Pr_base) |>
left_join(last_decade_annualy, by = "Station") |>
mutate(T_diff = Tavg_last_decade-Tavg_base,
Sn_diff = Sn_last_decade-Sn_base,
Pr_diff = Pr_last_decade-Pr_base) |>
mutate(across(c(T_anomaly, Pr_anomaly, Sn_anomaly, Tavg, Pr, Sn), list(
roll_mean = means_smooth
))) |>
mutate(across(contains("roll_mean"),
list(from_1976 = \(x) c(rep(NA, 15), x[16:length(x)])
)))
monthly_anomaly <- seven_station |>
left_join(base_monthly, by = c("Station", "Month")) |>
group_by(Station, Month) |>
mutate(T_anomaly=Tavg-Tavg_base,
Sn_anomaly=Sn-Sn_base,
Pr_anomaly=Pr-Pr_base) |>
left_join(last_decade_monthly, by=c("Station","Month"))|>
mutate(T_diff = Tavg_last_decade-Tavg_base,
Sn_diff = Sn_last_decade-Sn_base,
Pr_diff = Pr_last_decade-Pr_base) |>
mutate(across(c(T_anomaly, Pr_anomaly, Sn_anomaly, Tavg, Pr, Sn), list(
roll_mean = means_smooth
))) |>
mutate(across(contains("roll_mean"),
list(from_1976 = \(x) c(rep(NA, 15), x[16:length(x)])
)))
write_csv2(annualy_anomaly, "initial_data/climate/cleaned/annualy_anomaly.csv")
write_csv2(monthly_anomaly, "initial_data/climate/cleaned/monthly_anomaly.csv")
#| label: Загрузка пакетов
#| warning: false
library(tidyverse)
library(readxl)
#| label: Функции
#| warning: false
bad_snow_count <- function(df, quality){
# фильтрация данных с сентября по декабрь. Год прибавляет +1, так как впоследствии это количество будет суммироваться с данными следующего года (зимний период захватывает прошлый и последующий годы)
sep_dec <- df |>
filter(Month %in% c(9:12), Sn_description %in% quality) |>
group_by(Year, Station) |>
summarise(N = n()) |>
mutate(Year =Year+1)
# данные с января по май
jan_may <- df |>
filter(Month %in% c(1:5), Sn_description %in% quality) |>
group_by(Year, Station) |>
summarise(N = n())
# Джойн таблиц, заполнение пропусков нулями, расчет общего количества неблагоприятных дней за всю зиму
total <-  sep_dec |>
full_join(jan_may, by = c("Year", "Station")) |>
mutate(N.x=ifelse(is.na(N.x),0, N.x)) |>
mutate(N.y=ifelse(is.na(N.y),0, N.y)) |>
mutate(count = (N.x+N.y)) |>
rename(First_half = N.x, Last_half = N.y)
return(total)
}
#| label: Импорт
#| warning: false
# Import ------------------------------------------------------------------
snow_quality_data <- read_csv2("initial_data/climate/cleaned/Bakhta_daily_problem_with_summer_1966-1976.csv") |>
select(Year, Month, Day, Tavg, Sn_description)
# 2005 - 2023 -------------------------------------------------------------
snow_quality_2005_2023 <- snow_quality_data |>
filter(Year > 2005 | (Year == 2005 & Month >1))
levels(as.factor(snow_quality_data$Sn_description))
#| warning: false
s <- levels(as.factor(snow_quality_data$Sn_description))[c(16,20,17)] # Отбор уровней фактора, когда снег = лед, или снег не покрывает всю поверхность почвы
snow_quality_2005_2023 <- snow_quality_2005_2023 |>
mutate(Sn_description = ifelse(Sn_description %in% s,1,0)) |>  # 1 - если снег "плохой"
select(Year, Month, Day,Tavg,  Sn_description) |>
mutate(Station = "bakhta")
rm(s, snow_quality_data)
#| warning: false
bad_snow_bakhta <- rbind(bad_snow_count(snow_quality_1961_2004, quality = 1),
bad_snow_count(snow_quality_2005_2023, quality = 1)) |>
mutate(Station = "bakhta")
#| label: Загрузка пакетов
#| warning: false
library(tidyverse)
library(readxl)
library(MuMIn)
#| label: функции
#| warning: false
read_files  <- function(x){
read_excel(x) |>
select(Local_time, Temp = T, Sn_description = `E'`, sss) |>
mutate(Local_time = as_datetime(Local_time, format = "%d.%m.%Y %R")) |>
mutate(Year = year(Local_time),
Month = as.factor(month(Local_time)),
Day = mday(Local_time)) |>
filter(is.na(Temp) == F) |>
mutate(
more_then_zero = Temp > 0, # логический столбец, параметр больше 0? (TRUE | FALSE)
chng1 = cumsum(more_then_zero != lag(more_then_zero, def = first(more_then_zero)))
) |>
group_by(Year, Month, Day) |>
mutate(max_chng = max(chng1),
min_chng = min(chng1),
diff = max_chng-min_chng) |>
summarise(Sn = mean(sss, na.rm = T),
Sn_description = as.factor(str_flatten(Sn_description, ", ", na.rm = T)),
Tavg = mean(Temp, na.rm = T),
diff = max(diff))
}
daily_temp <- function(x){
read_excel(x) |>
select(Local_time, Temp = T, Sn_description = `E'`, sss) |>
mutate(Local_time = as_datetime(Local_time, format = "%d.%m.%Y %R")) |>
mutate(Year = year(Local_time),
Month = as.factor(month(Local_time)),
Day = mday(Local_time)) |>
filter(is.na(Temp) == F)
}
#| label: Импорт
#| warning: false
paths <- list.files("initial_data/climate/2005_2023", pattern = "[.]xls$", full.names = TRUE) # Просканировали все файлы в директории
total_2005_2023 <- paths |>
map_df(read_files)
s <- levels(as.factor(total_2005_2023$Sn_description))[c(4, 5,6)] # Отбор уровней фактора, когда снег = лед, или снег не покрывает всю поверхность почвы
total_2005_2023 <- total_2005_2023 |>
mutate(Sn_description = ifelse(Sn_description %in% s,1,0)) |>
mutate(Sn = case_when(is.na(Sn) ~ 0, .default = Sn))
total_2005_2023
